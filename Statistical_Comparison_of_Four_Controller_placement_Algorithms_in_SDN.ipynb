{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **One-Way ANOVA TEST**\n",
        "\n",
        "\n",
        "*The aim of this analysis is to statistically know if the proposed controller placementgorithm truely has a significant low output loss when it is compared with the reviewed Mopso, NSGA-II and NSGA-III algorithms. The importance of this analysi is to help to identify the most recommended controller placement alorithm with the least significant output loss in the SDN.* \n",
        "\n",
        "This link https://www.reneshbedre.com/blog/anova.html?utm_content=cmp-true explains more on the ANOVA test method.\n"
      ],
      "metadata": {
        "id": "9CQtEkO3i6hz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we will use bioinfokit (v1.0.3 or later) for performing tukey HSD and some other tests\n",
        "# check documentation here https://github.com/reneshbedre/bioinfokit\n",
        "!pip install bioinfokit"
      ],
      "metadata": {
        "id": "WYOo05UpPdcv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfe48e3d-9c56-4294-a8dc-f5f2e989a5b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: bioinfokit in /usr/local/lib/python3.9/dist-packages (2.1.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.9/dist-packages (from bioinfokit) (0.8.10)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from bioinfokit) (1.22.4)\n",
            "Requirement already satisfied: textwrap3 in /usr/local/lib/python3.9/dist-packages (from bioinfokit) (0.9.2)\n",
            "Requirement already satisfied: adjustText in /usr/local/lib/python3.9/dist-packages (from bioinfokit) (0.8)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from bioinfokit) (1.2.1)\n",
            "Requirement already satisfied: matplotlib-venn in /usr/local/lib/python3.9/dist-packages (from bioinfokit) (0.11.9)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from bioinfokit) (1.10.1)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.9/dist-packages (from bioinfokit) (0.13.5)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.9/dist-packages (from bioinfokit) (0.11.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from bioinfokit) (3.5.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from bioinfokit) (1.3.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->bioinfokit) (8.4.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->bioinfokit) (4.39.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->bioinfokit) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->bioinfokit) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->bioinfokit) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->bioinfokit) (3.0.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->bioinfokit) (23.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.9/dist-packages (from pandas->bioinfokit) (2022.7.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->bioinfokit) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->bioinfokit) (1.2.0)\n",
            "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.9/dist-packages (from statsmodels->bioinfokit) (0.5.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from patsy>=0.5.2->statsmodels->bioinfokit) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Conduct random sampling\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "input_value = input(\"Enter 1 to conduct sampling or 0 to use the documented 11 sample size dataset\")\n",
        "if (int(input_value) == 1):\n",
        "  sample_size_input = input(\"Enter any value greater than 9:\")\n",
        "  if (int(sample_size_input) >= 10):  \n",
        "\n",
        "      # Set the seed value for reproducibility\n",
        "      np.random.seed(123)\n",
        "\n",
        "      # load data file\n",
        "      #df = pd.read_excel(\"final output loss.xlsx\")\n",
        "      df=pd.read_excel('/content/drive/MyDrive/final output loss.xlsx')\n",
        "\n",
        "      #Sample the data:\n",
        "      #Note: I decided not to use (sample = df.sample(n=sample_size, random_state=123, replace=False)) code \n",
        "      #becuase the independent sampling of each data column will reduce the sampling biasness.\n",
        "      sample_size = int(sample_size_input)\n",
        "      proposed  = np.random.choice(df['proposed'], sample_size, replace=False)\n",
        "      mopso     = np.random.choice(df['mopso'], sample_size, replace=False)\n",
        "      nsga2     = np.random.choice(df['nsga2'], sample_size, replace=False)\n",
        "      nsga3     = np.random.choice(df['nsga3'], sample_size, replace=False)\n",
        "\n",
        "      # Combine the vectors back into a DataFrame\n",
        "      df = pd.DataFrame({'proposed': proposed, 'mopso': mopso, 'nsga2': nsga2, 'nsga3': nsga3})\n",
        "\n",
        "      #write the new data to textfile and save it\n",
        "      file_path = 'final output loss.csv'\n",
        "\n",
        "      with open(file_path, 'w') as file:\n",
        "          file.write(df['proposed'].to_string(index=False))\n",
        "        \n",
        "      # Print a confirmation message\n",
        "      print(f\"DataFrame written to file: {file_path}\")\n",
        "      print(df.tail(5))\n",
        "\n",
        "  else:\n",
        "    print(\"You entered the wrong value. The process will end now. Thnal you.\")\n",
        "elif (int(input_value) == 0):\n",
        "  #df = pd.read_csv(\"final output loss.txt\", sep=\"\\t\")\n",
        "  df=pd.read_csv(\"/content/drive/MyDrive/final output loss.txt\", sep=\"\\t\")\n",
        "  print(\"The built-in data of sample size 11 had been stored in the memory now.\")\n",
        "  print(df)\n",
        "else:\n",
        "  #The process ends\n",
        "  print(\"You entered the wrong ressponse. I am sorry that the process will be abordted now. \\n Kindly run the programme again. Thank you\")"
      ],
      "metadata": {
        "id": "3tVZqy2pkwz9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "outputId": "3584d4ef-146d-427a-8735-6c166308fb23"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter 1 to conduct sampling or 0 to use the documented 11 sample size dataset1\n",
            "Enter any value greater than 9:150\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-7c699afe5882>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m       \u001b[0;31m# load data file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m       \u001b[0;31m#df = pd.read_excel(\"final output loss.xlsx\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m       \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/final output loss.xlsx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0;31m#Sample the data:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[1;32m   1189\u001b[0m                 \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xls\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m                 ext = inspect_excel_format(\n\u001b[0m\u001b[1;32m   1192\u001b[0m                     \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1068\u001b[0m         \u001b[0mcontent_or_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1070\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m   1071\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m     ) as handle:\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    712\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/final output loss.xlsx'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Importation**\n",
        "\n",
        "*The network final output loss data for this analysis, with four columns, is importated below.*"
      ],
      "metadata": {
        "id": "A9eUYZ2rljDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load data file\n",
        "#\n",
        "\n",
        "# reshape the d dataframe suitable for statsmodels package \n",
        "df_melt = pd.melt(df.reset_index(), id_vars=['index'], value_vars=['proposed', 'mopso', 'nsga2', 'nsga3'])\n",
        "\n",
        "#print(df_melt)\n",
        "\n",
        "# replace column names\n",
        "df_melt.columns = ['index', 'treatments', 'value']\n",
        "\n",
        "#print(df['proposed'])"
      ],
      "metadata": {
        "id": "ZPMpcDMQjkmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "## **The four controllers placement algorithms means**\n",
        "\n",
        "*The network final output loss means for the four controllers placement algorithms (the Proposed algorithms, MOPSO, NSGA-II and NSGA-III algorithms) in the appropriate placement of controller in the SDN are:*"
      ],
      "metadata": {
        "id": "I3fIMMlspa4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The network final output loss means of the algorithms are: \\n\",df.mean())\n",
        "print(\"\\n ============================ \\n\")\n",
        "print(\"The network final output loss variances of the algorithms are: \\n\",df.var())"
      ],
      "metadata": {
        "id": "97AqjcTiosjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Interpretation:**\n",
        "\n",
        "This means that the proposed algorithm has the network final output loss means of 943.540408 while the MOPSO algorithms has the highest network final output loss means of 1459.861607. This study is interested to know if there is significant difference among these fours controller based on the network final output loss data. *"
      ],
      "metadata": {
        "id": "fVsz8Sj8qAHt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mYFUP94_U9vJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "\n",
        "# **Box plot Presentation:**\n",
        "\n",
        "*The Box plot is plotted below.*"
      ],
      "metadata": {
        "id": "XSRpnyPRjqAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate a boxplot to see the data distribution by treatments. Using boxplot, we can \n",
        "# easily detect the differences between different treatments\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "ax = sns.boxplot(x='treatments', y='value', data=df_melt, color='#99c2a2')\n",
        "ax = sns.swarmplot(x=\"treatments\", y=\"value\", data=df_melt, color='#7d0013')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EkSRMB7vBvOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation of the Box plot for the four controllers:**\n",
        "\n",
        "Each box represents each controller. There seems to be complications on thise plot. I hope to inestigate further and later.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "iG2NruoUj4Bq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **One Way ANOVA Test**\n",
        "\n",
        "*Below id the one-way anova test analysis and results.*\n",
        "\n",
        "H0: There are no significant differences among final network loss mean produced by the four algorithms at 5% significant level.\n",
        "\n",
        "H1: At least, one of the algorithms has final network loss mean that is different from others at 5% significant level."
      ],
      "metadata": {
        "id": "cz4UEM8zkRxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats as stats\n",
        "# stats f_oneway functions takes the groups as input and returns ANOVA F and p value\n",
        "fvalue, pvalue = stats.f_oneway(df['proposed'], df['mopso'], df['nsga2'], df['nsga3'])\n",
        "print(fvalue, pvalue)\n",
        "\n",
        "print()\n",
        "if (pvalue < 0.05):\n",
        "  print(\" Decision: Reject the Null hypothesis \\n At least, one of the algorithms has final network loss mean that is different from others at 5% significant level.\")\n",
        "else:\n",
        "  print(\" Decision: Accept the Null hypothesis \\n There are no significant differences among final network loss mean produced by the four algorithms at 5% significant level.\")\n",
        "\n",
        "print()\n",
        "print(\"==================================================================\")\n",
        "\n",
        "# get ANOVA table as R like output\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "\n",
        "# Ordinary Least Squares (OLS) model\n",
        "model = ols('value ~ C(treatments)', data=df_melt).fit()\n",
        "anova_table = sm.stats.anova_lm(model, typ=2)\n",
        "anova_table\n",
        "\n",
        "print()\n",
        "print(\"==================================================================\")\n",
        "\n",
        "# ANOVA table using bioinfokit v1.0.3 or later (it uses wrapper script for anova_lm)\n",
        "from bioinfokit.analys import stat\n",
        "res = stat()\n",
        "res.anova_stat(df=df_melt, res_var='value', anova_model='value ~ C(treatments)')\n",
        "res.anova_summary"
      ],
      "metadata": {
        "id": "EtQtcWmcKq2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ANOVA TEst Result Interpretation**\n",
        "*The p value obtained from ANOVA analysis is significant (p < 0.05), and therefore, we conclude that there are significant differences among treatments.*\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "m8La8yBfURz4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Post HOC TesT**\n",
        "\n",
        "*Below is Post- HOC Test using Tukey's HSD test.*"
      ],
      "metadata": {
        "id": "Ogfnu7UWkw_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bioinfokit.analys import stat\n",
        "# perform multiple pairwise comparison (Tukey's HSD)\n",
        "# unequal sample size data, tukey_hsd uses Tukey-Kramer test\n",
        "res = stat()\n",
        "res.tukey_hsd(df=df_melt, res_var='value', xfac_var='treatments', anova_model='value ~ C(treatments)')\n",
        "res.tukey_summary"
      ],
      "metadata": {
        "id": "K4dMNzyRR-g_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation of Post-HOC:** *Above results from Tukey’s HSD suggests that except A-C, all other pairwise comparisons for controller location means rejects null hypothesis (p < 0.05) and indicates statistical significant differences between the pair of final output loss.*\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "uXi5KiD9TrZW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Normal distribution:**\n",
        "\n"
      ],
      "metadata": {
        "id": "EAzsc4CFdbGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Test ANOVA assumptionsPermalink\n",
        "#ANOVA assumptions can be checked using test statistics (e.g. Shapiro-Wilk, Bartlett’s, Levene’s test, Brown-Forsythe test) \n",
        "#and the visual approaches such as residual plots (e.g. QQ-plots) and histograms.\n",
        "\n",
        "#The visual approaches perform better than statistical tests. For example, the Shapiro-Wilk test has low power for small \n",
        "#sample size data and deviates significantly from normality for large sample sizes (say n > 50). For large sample sizes, you \n",
        "#should consider to use QQ-plot for normality assumption.\n",
        "\n",
        "# QQ-plot\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "# res.anova_std_residuals are standardized residuals obtained from ANOVA (check above)\n",
        "sm.qqplot(res.anova_std_residuals, line='45')\n",
        "plt.xlabel(\"Theoretical Quantiles\")\n",
        "plt.ylabel(\"Standardized Residuals\")\n",
        "plt.show()\n",
        "\n",
        "# histogram\n",
        "plt.hist(res.anova_model_out.resid, bins='auto', histtype='bar', ec='k') \n",
        "plt.xlabel(\"Residuals\")\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0OYPHIkfQaw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation of standardized residuals plot:**\n",
        "*As the standardized residuals lie closely around the 45-degree line, it suggests that the residuals are are likely to be approximately normally distributed.*\n",
        "\n",
        "**Interpretation of histogram plot:**\n",
        "In the histogram, the distribution looks approximately left tailed and suggests that residuals are not like to be approximately normally distributed. \n",
        "\n",
        "The normal distrinution and homogenuity teste will be conducted for confirmation of what we suspected in the graph.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "hKWd3Q1SVCCr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Shapiro-Wilk test for Normality assumption test:**\n",
        "\n",
        "H0: The dataset is drawn from normal distribution at 5% significant level.\n",
        "\n",
        "H1: The dataset is not drawn from normal distribution at 5% significant level."
      ],
      "metadata": {
        "id": "YmAHK18MdsM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Shapiro-Wilk test can be used to check the normal distribution of residuals. Null hypothesis: data is drawn from normal distribution.\n",
        "\n",
        "import scipy.stats as stats\n",
        "w, pvalue = stats.shapiro(model.resid)\n",
        "print(w, pvalue)\n",
        "\n",
        "print(\"==============================\")\n",
        "\n",
        "if (pvalue > 0.05):\n",
        "  print(\" Decision: Accept the Null hypothesis \\n Interpretation: The data is drawn from normal distribution at 5% significant level\")\n",
        "else:\n",
        "  print(\" Decision: Reject the Null hypothesis \\n Interpretation: The data is NOT drawn from normal distribution at 5% significant level\")"
      ],
      "metadata": {
        "id": "bXGByt97aDGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vxdvNyTKgqmt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation Shapiro-Wilk test:**\n",
        "*As the p value (0.6609 > 0.05) is significant, we accept the null hypothesis and conclude that data is drawn from normal distribution.*\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "dTmHz-76aQvF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Homogeneity of variances test:**\n",
        "\n",
        "**H0:** The four Samples, from the populations, have equal variances.\n",
        "\n",
        "**H1:** The four Samples, from the populations, do not have equal variances."
      ],
      "metadata": {
        "id": "2mNdTrg4cGbA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bartlett's test**\n",
        "*Below is bartlett's test for equality of variance test.*"
      ],
      "metadata": {
        "id": "L0BaFUpdhdmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats as stats\n",
        "w, pvalue = stats.bartlett(df['proposed'], df['mopso'], df['nsga2'], df['nsga3'])\n",
        "print(w, pvalue)\n",
        "\n",
        "# if you have a stacked table, you can use bioinfokit v1.0.3 or later for the bartlett's test\n",
        "from bioinfokit.analys import stat \n",
        "res = stat()\n",
        "res.bartlett(df=df_melt, res_var='value', xfac_var='treatments')\n",
        "res.bartlett_summary\n",
        "#res.bartlett_summary.values"
      ],
      "metadata": {
        "id": "O6NwgWbKbsMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Levene's test**\n",
        "*Below is levene's test for equality of variance test.*"
      ],
      "metadata": {
        "id": "RGvvuBcnh2KV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# if you have a stacked table, you can use bioinfokit v1.0.3 or later for the Levene's test\n",
        "from bioinfokit.analys import stat \n",
        "res = stat()\n",
        "res.levene(df=df_melt, res_var='value', xfac_var='treatments')\n",
        "res.levene_summary"
      ],
      "metadata": {
        "id": "TwmAL4CihXii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation:**\n",
        "\n",
        "*As the p value, for both **Bartlett's test (0.0011 < 0.05) and levene test (0.0130 < 0.05)** is significant, we reject the null hypothesis and conclude that four algorithms do not have equal variances.*\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "fqMZln30fkaO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Kruskal-Wallis test**\n",
        "\n",
        "This test replaces the ANOVA test dues to the violation of the homoscedastisity of variance assumption.\n",
        "\n",
        "HO: There is no significant differences among network final output loss medians for the four algorithms at 5% significant level.\n",
        "\n",
        "H1: At least one of the algorithms produces the network final output loss median that is significantly different from other algorithms at 5% significant level.\n",
        "\n"
      ],
      "metadata": {
        "id": "EvVGOUvlGTzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import kruskal\n",
        "\n",
        "# Separate the data into the groups based on the fourth column\n",
        "proposed_algorithm = df['proposed']\n",
        "mopso_algorithm = df['mopso']\n",
        "nsga2_algorithm = df['nsga2']\n",
        "nsga3_algorithm = df['nsga3']\n",
        "\n",
        "# Conduct the Kruskal-Wallis test on the groups\n",
        "stat, p = kruskal(proposed_algorithm, mopso_algorithm, nsga2_algorithm, nsga3_algorithm)\n",
        "\n",
        "# Print the results\n",
        "print('Kruskal-Wallis Test')\n",
        "print('-------------------')\n",
        "print('H statistic =', stat)\n",
        "print('p value =', p)\n",
        "\n",
        "print()\n",
        "\n",
        "# If the p-value is less than the significance level (usually 0.05), reject the null hypothesis\n",
        "if p < 0.05:\n",
        "    print(' Decision: There is significant evidence to reject the null hypothesis at 5% significant level. \\n Conclusion: At least one of the algorithms produces the network final output loss median that is significantly different from other algorithms at 5% significant level')\n",
        "else:\n",
        "    print('There is not enough evidence to reject the null hypothesis at 5% significant level. \\n Conclusion: There is no significant differences among network final output loss medians for the four algorithms at 5% significant level.')"
      ],
      "metadata": {
        "id": "abgxb7G2I9pU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Since there is evidence that at least one of the algorithms produces the network final output loss median that is significantly different from other algorithms at 5% significant level, hence the test will compare the lagorithms pairwisely using the Pairwise Wilcoxon rank-sum test.**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## **Pairwise Wilcoxon rank-sum test**"
      ],
      "metadata": {
        "id": "kC3w6JC4MhC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import ranksums\n",
        "\n",
        "# data for groups A, B, and C\n",
        "print(\"The median will be compared for difference. The median for the final output loss for:\")\n",
        "print(\"Proposed Algorithm   :\", df['proposed'].median())\n",
        "print(\"Mopso Algorithm      :\", df['mopso'].median())\n",
        "print(\"NSGA-II Algorithm    :\", df['nsga2'].median())\n",
        "print(\"NSGA-III Algorithm   :\", df['nsga3'].median())\n",
        "print()\n",
        "\n",
        "# pairwise comparisons of groups\n",
        "print(\"Proposed Algorithm and Mopso Algorithm Comparism\")\n",
        "print(\"================================================\")\n",
        "print(ranksums(proposed_algorithm, mopso_algorithm))\n",
        "pvalue1 = ranksums(proposed_algorithm, mopso_algorithm).pvalue\n",
        "if (pvalue1 < 0.05):\n",
        "  print(\"The null hypothesis is rejected. It is concluded that there is significant difference \\n between the two median values of the two compared Algorithms at 5% signifiant level. \\n This implies that the proposed algorithm has significant low final output loss than the Mopso algorithm at 5% significant level.\")\n",
        "else:\n",
        "  print(\"The null hypothesis is accepted. It is concluded that there is no significant difference \\n between the two median values of the compared two Algorithms at 5% signifiant level. \\n This implies that any obseved difference between the medians of the two compared algorithms is not significant at 5% significant level.\")\n",
        "\n",
        "print()\n",
        "print(\"Proposed Algorithm and NSGA-II Algorithm Comparism\")\n",
        "print(\"================================================\")\n",
        "print(ranksums(proposed_algorithm, nsga2_algorithm))\n",
        "pvalue2 = ranksums(proposed_algorithm, nsga2_algorithm).pvalue\n",
        "if (pvalue2 < 0.05):\n",
        "  print(\"The null hypothesis is rejected. It is concluded that there is significant difference \\n between the two median values of the two compared Algorithms at 5% signifiant level. \\n This implies that the proposed algorithm has significant low final output loss than the Mopso algorithm at 5% significant level.\")\n",
        "else:\n",
        "  print(\"The null hypothesis is accepted. It is concluded that there is no significant difference \\n between the two median values of the compared two Algorithms at 5% signifiant level. \\n This implies that any obseved difference between the medians of the two compared algorithms is not significant at 5% significant level.\")\n",
        "\n",
        "print()\n",
        "print(\"Proposed Algorithm and NSGA-III Algorithm Comparism\")\n",
        "print(\"================================================\")\n",
        "print(ranksums(proposed_algorithm, nsga3_algorithm))\n",
        "pvalue3 = ranksums(proposed_algorithm, nsga3_algorithm).pvalue\n",
        "if (pvalue3 < 0.05):\n",
        "  print(\"The null hypothesis is rejected. It is concluded that there is significant difference \\n between the two median values of the two compared Algorithms at 5% signifiant level. \\n This implies that the proposed algorithm has significant low final output loss than the Mopso algorithm at 5% significant level.\")\n",
        "else:\n",
        "  print(\"The null hypothesis is accepted. It is concluded that there is no significant difference \\n between the two median values of the compared two Algorithms at 5% signifiant level. \\n This implies that any obseved difference between the medians of the two compared algorithms is not significant at 5% significant level.\")\n",
        "\n",
        "print()\n",
        "print(\"NSGA-II Algorithm and NSGA-III Algorithm Comparism\")\n",
        "print(\"================================================\")\n",
        "print(ranksums(nsga2_algorithm, nsga3_algorithm))\n",
        "pvalue4 = ranksums(nsga2_algorithm, nsga3_algorithm).pvalue\n",
        "if (pvalue4 < 0.05):\n",
        "  print(\"The null hypothesis is rejected. It is concluded that there is significant difference \\n between the two median values of the two compared Algorithms at 5% signifiant level. \\n This implies that the proposed algorithm has significant low final output loss than the Mopso algorithm at 5% significant level.\")\n",
        "else:\n",
        "  print(\"The null hypothesis is accepted. It is concluded that there is no significant difference \\n between the two median values of the compared two Algorithms at 5% signifiant level. \\n This implies that any obseved difference between the medians of the two compared algorithms is not significant at 5% significant level.\")\n",
        "\n",
        "print()\n",
        "print(\"Mopso Algorithm and NSGA-III Algorithm Comparism\")\n",
        "print(\"================================================\")\n",
        "print(ranksums(mopso_algorithm, nsga3_algorithm))\n",
        "pvalue5 = ranksums(mopso_algorithm, nsga3_algorithm).pvalue\n",
        "if (pvalue5 < 0.05):\n",
        "  print(\"The null hypothesis is rejected. It is concluded that there is significant difference \\n between the two median values of the two compared Algorithms at 5% signifiant level. \\n This implies that the proposed algorithm has significant low final output loss than the Mopso algorithm at 5% significant level.\")\n",
        "else:\n",
        "  print(\"The null hypothesis is accepted. It is concluded that there is no significant difference \\n between the two median values of the compared two Algorithms at 5% signifiant level. \\n This implies that any obseved difference between the medians of the two compared algorithms is not significant at 5% significant level.\")\n",
        "\n",
        "print()\n",
        "print(\"Mopso Algorithm and NSGA-II Algorithm Comparism\")\n",
        "print(\"================================================\")\n",
        "print(ranksums(mopso_algorithm, nsga2_algorithm))\n",
        "pvalue6 = ranksums(mopso_algorithm, nsga2_algorithm).pvalue\n",
        "if (pvalue6 < 0.05):\n",
        "  print(\"The null hypothesis is rejected. It is concluded that there is significant difference \\n between the two median values of the two compared Algorithms at 5% signifiant level. \\n This implies that the proposed algorithm has significant low final output loss than the Mopso algorithm at 5% significant level.\")\n",
        "else:\n",
        "  print(\"The null hypothesis is accepted. It is concluded that there is no significant difference \\n between the two median values of the compared two Algorithms at 5% signifiant level. \\n This implies that any obseved difference between the medians of the two compared algorithms is not significant at 5% significant level.\")\n"
      ],
      "metadata": {
        "id": "aTSkhT-8NJBh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}